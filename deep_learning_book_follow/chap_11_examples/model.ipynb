{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# IMDB Movie Review Sentiment Analysis\n",
    "This model will predict sentiments from IMDB movie reviews.\n",
    "- data is taken from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz and extracted\n",
    "- training data is placed in aclImdb; subdivided into train/neg, train/pos, test/neg, and test/pos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import os, pathlib, shutil, random\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# base, training, and validation directory\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "train_dir = base_dir / \"train\"\n",
    "val_dir = base_dir / \"val\"\n",
    "test_dir = base_dir / \"test\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ONLY RUN ONCE - Split 20% of the training data into validation data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the ratio of files to move\n",
    "rat = 0.2\n",
    "\n",
    "# loop through each category\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    # shuffle the files in the category in the training data\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val = int(rat * len(files))\n",
    "\n",
    "    # move the first [num_val] into the validation set\n",
    "    for i in range(num_val):\n",
    "        shutil.move(train_dir / category / files[i], val_dir / category / files[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(train_dir, batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(test_dir, batch_size = batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(val_dir, batch_size=batch_size)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_batch(dataset: tf.data.Dataset):\n",
    "    # taking a look at the inputs and targets in our batch\n",
    "    for inputs, targets in dataset:\n",
    "        print(f\"Input Shape: {inputs.shape}\")\n",
    "        print(f\"Input Type: {inputs.dtype}\")\n",
    "        print(f\"Target Shape: {targets.shape}\")\n",
    "        print(f\"Target Type: {targets.dtype}\")\n",
    "        print(f\"Inputs[0]: {inputs[0]}\")\n",
    "        print(f\"Targets[0]: {targets[0]}\")\n",
    "        break\n",
    "display_batch(train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bag-of-words approach\n",
    "\n",
    "Arguably easiest, we disregard the order and just look at the \"existance\" of words\n",
    "\n",
    "### We start with a 1-gram bag-of-words approach"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the vectorization layer and add the vocabulary from our training dataset\n",
    "vocab_size = 20000\n",
    "text_vectorization = layers.TextVectorization(output_mode=\"multi_hot\", max_tokens=vocab_size)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# vectorize the input in each\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "display_batch(binary_1gram_train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model(max_tokens=vocab_size, hidden_dim=16):\n",
    "    # create a model, compiled and all\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model = get_model()\n",
    "history = model.fit(binary_1gram_train_ds, epochs=10, validation_data=binary_1gram_val_ds.cache(),\n",
    "                    callbacks=[keras.callbacks.ModelCheckpoint(filepath=\"binary_1gram.keras\", save_best_only=True)])\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(model.evaluate(binary_1gram_test_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bi (2) gram Text Vectorization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# redefine text vectorization\n",
    "text_vectorization = layers.TextVectorization(max_tokens=vocab_size, ngrams=2, output_mode=\"multi_hot\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# remap our data into the encoding\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "display_batch(binary_2gram_train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(filepath=\"binary_2gram.keras\", save_best_only=True)]\n",
    "model.fit(binary_2gram_train_ds, validation_data=binary_2gram_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(model.evaluate(binary_2gram_test_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bigram Count Approach\n",
    "This time, not only will we take a 2-gram of the words, but also take note of their frequency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# another text vectorization\n",
    "text_vectorization = layers.TextVectorization(max_tokens=vocab_size, ngrams=2, output_mode=\"tf_idf\")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# map the datasets\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "display_batch(tfidf_2gram_train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train and evaluate the models\n",
    "model = get_model()\n",
    "model.fit(tfidf_2gram_train_ds, epochs=10, validation_data=tfidf_2gram_val_ds,\n",
    "          callbacks=[keras.callbacks.ModelCheckpoint(filepath=\"tfidf_2gram.keras\", save_best_only=True)])\n",
    "print(model.evaluate(tfidf_2gram_test_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inferring a Model\n",
    "\n",
    "In a production environment, it is best to include the preprocessing into the model itself"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_input = text_vectorization(inputs)\n",
    "outputs = model(processed_input)\n",
    "\n",
    "prod_model = keras.Model(inputs, outputs)\n",
    "\n",
    "raw_text_data = tf.convert_to_tensor([[\"That was an excellent movie! I loved it\"], [\"It was a horrible movie. I hated it\"]])\n",
    "print(raw_text_data)\n",
    "predictions = prod_model(raw_text_data)\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sequential Models\n",
    "This time, instead of doing feature engineering, we allow a recursive model learn the results themselves"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare integer sequences for each word; we will still use data variables from the first attempt\n",
    "vocab_size = 20000\n",
    "max_length = 600\n",
    "text_vectorization = layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=max_length)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=16)\n",
    "\n",
    "display_batch(int_train_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's start with a one-hot bidirectional lstm layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = keras.Input((None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=vocab_size)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(int_train_ds, epochs=10, validation_data=int_val_ds,\n",
    "          callbacks=[keras.callbacks.ModelCheckpoint(filepath=\"one_hot_bidir_lstm.keras\", save_best_only=True)])\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(model.evaluate(test_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding Embeddings\n",
    "Embeddings convert our words into a vector - rather than a one_hot, it compresses the data into a much smaller vector\n",
    "- Semantic relationships become geometric - \"closer\" words are more similar, different types of relationships can often be represented as a vector translation\n",
    "- These relationships are learned\n",
    "- the mask_zero parameter adds a mask\n",
    "    - Without it, there are a bunch of zeros at the end which will dillute the information that the forward Recursive layer can extract from the actually significant parts of the sentence\n",
    "    - It propogates through the entire model, each layer has access to the mask\n",
    "    - lstm's automatically only return the last nonzero result, and other layers will make sure, if a full sequence is returned, to only take the last nonzero (important) word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = keras.Input((max_length,))\n",
    "embedded = layers.Embedding(input_dim=vocab_size, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "          callbacks=[keras.callbacks.ModelCheckpoint(filepath=\"embeddings_bidir_gru.keras\", save_best_only=True)])\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "print(model.evaluate(int_test_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pretrained Word Embeddings\n",
    "- Using the GloVe embeddings (taken from https://nlp.stanford.edu/data/glove.6B.zip)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_glove = pathlib.Path(\"glove.6B/glove.6B.100d.txt\")\n",
    "word_to_embedding = {}\n",
    "with open(path_to_glove, \"rt\", encoding=\"utf8\") as f:\n",
    "    for i in f:\n",
    "        word, coefs = i.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, sep=\" \")\n",
    "        assert len(coefs) == 100\n",
    "        word_to_embedding[word] = coefs\n",
    "\n",
    "print(f\"Found {len(word_to_embedding)} words\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = {vocabulary[i]: i for i in range(len(vocabulary))}\n",
    "embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size and word in word_to_embedding:\n",
    "        embedding_matrix[i] = word_to_embedding[word]\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = keras.Input((None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "callbacks=[keras.callbacks.ModelCheckpoint(filepath=\"glove_embeddings_sequence_model.keras\", save_best_only=True)])\n",
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(model.evaluate(int_test_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer Encoder\n",
    "- A layer with a MultiHeadAttention layer, Dense projections afterward, normalization, and residual connections\n",
    "- MultiHeadAttention layers consist of many heads\n",
    "    - each head stores one kind of \"relationship\" between words, and everything is concatenated together"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    A transformer encoder runs the input through a MultiHeadAttention layer and then a few Dense layers, performing various optimizations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        \"\"\"Transformer __init__ method\\n\n",
    "            embed_dim: the dimension of each word\\n\n",
    "            dense_dim: the dimension of the intermediate Dense layer\\n\n",
    "            num_heads: the number of heads in the multiheadattention layer\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # set constants\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim)])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        \"\"\"Calls the model; inputs.shape = (batch size, sequence length, embedding size)\"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        # dense layers are run independently for the last dimension - so it is run for each word in proj_input\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Used to get the constructor arguments, so that loading the model will re-initialize this object properly\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim\n",
    "        })\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(embedded)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=[keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\", save_best_only=True)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the custom_objects is required so that they know which class the Custom Layer comes from\n",
    "model = keras.models.load_model(\"transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test Accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, let's add a positional embedding - alter the embedding, and add an embedding of the current sequence position"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.word_embedding = layers.Embedding(input_dim, embed_dim)\n",
    "        self.position_embedding = layers.Embedding(sequence_length, embed_dim)\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        pos = tf.range(start=0, limit=length, delta=1)\n",
    "        return self.word_embedding(inputs) + self.position_embedding(pos)\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sequence_length = 600\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(embedded)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=[keras.callbacks.ModelCheckpoint(\"transformer_positional_encoder.keras\", save_best_only=True)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"transformer_positional_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder, \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test Accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}