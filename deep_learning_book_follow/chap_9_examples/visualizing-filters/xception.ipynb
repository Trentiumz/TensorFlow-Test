{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualizing Convnet Filters\n",
    "### Details\n",
    "- This file will experiment with the Xception convolutional base by finding the patterns for which its filters are most receptive to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\danie\\pycharmprojects\\tensorflowtest\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\danie\\pycharmprojects\\tensorflowtest\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\users\\danie\\pycharmprojects\\tensorflowtest\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\users\\danie\\pycharmprojects\\tensorflowtest\\venv\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1\n",
      "block1_conv2\n",
      "block2_sepconv1\n",
      "block2_sepconv2\n",
      "conv2d\n",
      "block3_sepconv1\n",
      "block3_sepconv2\n",
      "conv2d_1\n",
      "block4_sepconv1\n",
      "block4_sepconv2\n",
      "conv2d_2\n",
      "block5_sepconv1\n",
      "block5_sepconv2\n",
      "block5_sepconv3\n",
      "block6_sepconv1\n",
      "block6_sepconv2\n",
      "block6_sepconv3\n",
      "block7_sepconv1\n",
      "block7_sepconv2\n",
      "block7_sepconv3\n",
      "block8_sepconv1\n",
      "block8_sepconv2\n",
      "block8_sepconv3\n",
      "block9_sepconv1\n",
      "block9_sepconv2\n",
      "block9_sepconv3\n",
      "block10_sepconv1\n",
      "block10_sepconv2\n",
      "block10_sepconv3\n",
      "block11_sepconv1\n",
      "block11_sepconv2\n",
      "block11_sepconv3\n",
      "block12_sepconv1\n",
      "block12_sepconv2\n",
      "block12_sepconv3\n",
      "block13_sepconv1\n",
      "block13_sepconv2\n",
      "conv2d_3\n",
      "block14_sepconv1\n",
      "block14_sepconv2\n"
     ]
    }
   ],
   "source": [
    "# load in model\n",
    "model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# find the list of significant layers (Conv2D and SeparableConv2D layers)\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (layers.Conv2D, layers.SeparableConv2D)):\n",
    "        print(layer.name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Filter 0\n",
      "Processing Filter 1\n",
      "Processing Filter 2\n",
      "Processing Filter 3\n",
      "Processing Filter 4\n",
      "Processing Filter 5\n",
      "Processing Filter 6\n",
      "Processing Filter 7\n",
      "Processing Filter 8\n",
      "Processing Filter 9\n",
      "Processing Filter 10\n",
      "Processing Filter 11\n",
      "Processing Filter 12\n",
      "Processing Filter 13\n",
      "Processing Filter 14\n",
      "Processing Filter 15\n",
      "Processing Filter 16\n",
      "Processing Filter 17\n",
      "Processing Filter 18\n",
      "Processing Filter 19\n",
      "Processing Filter 20\n",
      "Processing Filter 21\n",
      "Processing Filter 22\n",
      "Processing Filter 23\n",
      "Processing Filter 24\n",
      "Processing Filter 25\n",
      "Processing Filter 26\n",
      "Processing Filter 27\n",
      "Processing Filter 28\n",
      "Processing Filter 29\n",
      "Processing Filter 30\n",
      "Processing Filter 31\n",
      "Processing Filter 32\n",
      "Processing Filter 33\n",
      "Processing Filter 34\n",
      "Processing Filter 35\n",
      "Processing Filter 36\n",
      "Processing Filter 37\n",
      "Processing Filter 38\n",
      "Processing Filter 39\n",
      "Processing Filter 40\n",
      "Processing Filter 41\n",
      "Processing Filter 42\n",
      "Processing Filter 43\n",
      "Processing Filter 44\n",
      "Processing Filter 45\n",
      "Processing Filter 46\n",
      "Processing Filter 47\n",
      "Processing Filter 48\n",
      "Processing Filter 49\n",
      "Processing Filter 50\n",
      "Processing Filter 51\n",
      "Processing Filter 52\n",
      "Processing Filter 53\n",
      "Processing Filter 54\n",
      "Processing Filter 55\n",
      "Processing Filter 56\n",
      "Processing Filter 57\n",
      "Processing Filter 58\n",
      "Processing Filter 59\n",
      "Processing Filter 60\n",
      "Processing Filter 61\n",
      "Processing Filter 62\n",
      "Processing Filter 63\n"
     ]
    }
   ],
   "source": [
    "# create a feature extraction model\n",
    "layer_name = \"block6_sepconv3\"\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)\n",
    "\n",
    "# a function that computes the loss\n",
    "def compute_loss(image, filter_index):\n",
    "    activation = feature_extractor(image)\n",
    "    filter_activation = activation[0,2:-2, 2:-2,filter_index]\n",
    "    return tf.reduce_mean(filter_activation)\n",
    "\n",
    "# perform one step of gradient ascent - note that normalizing the gradients ensures that the updates are a smaller range\n",
    "@tf.function\n",
    "def gradient_ascent_step(image, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(image, filter_index)\n",
    "    grads = tape.gradient(loss, image)\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    image.assign_add(learning_rate * grads)\n",
    "    return image\n",
    "\n",
    "# generates a filter pattern by continually running gradient ascent on on the filter\n",
    "img_width, img_height = 200, 200\n",
    "def generate_filter_pattern(filter_index):\n",
    "    iterations = 30\n",
    "    learning_rate = 10.\n",
    "    image = tf.random.uniform(minval=0, maxval=0, shape=(1, img_width, img_height, 3))\n",
    "    for i in range(iterations):\n",
    "        image = gradient_ascent_step(tf.Variable(image), filter_index, learning_rate)\n",
    "    return image[0].numpy()\n",
    "\n",
    "# makes an image fit for viewing\n",
    "def deprocess_image(image: np.ndarray):\n",
    "    image = (image - image.mean()) / image.std()\n",
    "    image = image * 64 + 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return image\n",
    "\n",
    "all_images = []\n",
    "for filter_index in range(min(64, layer.output_shape[-1])):\n",
    "    print(f\"Processing Filter {filter_index}\")\n",
    "    image = deprocess_image(generate_filter_pattern(filter_index=filter_index))\n",
    "    all_images.append(image)\n",
    "\n",
    "margin = 5\n",
    "n = 8\n",
    "cropped_width = img_width - 25 * 2\n",
    "cropped_height = img_height - 25 * 2\n",
    "width = n * cropped_width + (n-1) * margin\n",
    "height = n * cropped_height + (n-1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i * n + j >= len(all_images):\n",
    "            break\n",
    "        image = all_images[i * n + j]\n",
    "        stitched_filters[\n",
    "            (cropped_width+margin) * i:(cropped_width+margin)*i + cropped_width,\n",
    "            (cropped_height+margin)*j:(cropped_height+margin)*j+cropped_height, :\n",
    "        ] = image\n",
    "keras.utils.save_img(f\"filters_for_layer_{layer_name}.png\", stitched_filters)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}